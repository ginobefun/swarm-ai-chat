import { NextRequest, NextResponse } from 'next/server'
import { createOpenRouter } from '@openrouter/ai-sdk-provider'
import { streamText } from 'ai'
import { addMessageToSession } from '@/lib/database/sessions-prisma'
import { z } from 'zod'
import { ContextManager } from '@/lib/langchain/context-manager'
import { HumanMessage, AIMessage, SystemMessage } from '@langchain/core/messages'
import { globalMetricsTracker, createMetric } from '@/lib/metrics/agent-metrics'

// Types
interface ChatMessage {
    role: 'user' | 'assistant' | 'system'
    content: string
    parts?: Array<{
        type: string
        text: string
    }>
}

// Request validation schema
const ChatRequestSchema = z.object({
    sessionId: z.string().uuid(),
    message: z.string().min(1),
    agentId: z.string().optional(),
    userId: z.string()
})

/**
 * POST /api/chat
 * Handle chat messages with AI using OpenRouter's Gemini 2.5 Flash
 * 
 * Features:
 * - Stream AI responses using Vercel AI SDK + OpenRouter
 * - Save user and AI messages to SwarmChatMessage
 * - Support for different AI agents
 * - Error handling and logging
 */
export async function POST(req: NextRequest) {
    try {
        console.log('ğŸ” Chat API - Starting request processing')
        const body = await req.json()
        console.log('ğŸ” Chat API - Received body:', {
            hasMessages: !!body.messages,
            messagesLength: body.messages?.length,
            sessionId: body.sessionId,
            userId: body.userId,
            agentId: body.agentId
        })

        // Extract message from Vercel AI SDK format (messages array) or direct format
        const messageContent = body.messages?.slice(-1)[0]?.content || body.message
        console.log('ğŸ” Chat API - Extracted message content:', messageContent?.substring(0, 100) + '...')

        // Get all messages for context (required by AI SDK)
        const allMessages = body.messages || []
        console.log('ğŸ” Chat API - All messages count:', allMessages.length)

        // Validate request data (message field is not required since it comes from messages array)
        const { sessionId, agentId = 'gemini-flash', userId } = ChatRequestSchema.omit({ message: true }).parse(body)
        console.log('ğŸ” Chat API - Validation passed:', { sessionId, agentId, userId })

        // Validate message content
        if (!messageContent || typeof messageContent !== 'string') {
            throw new Error('Message content is required')
        }

        // Save user message to database first
        console.log('ğŸ” Chat API - Saving user message to database')
        try {
            await addMessageToSession({
                sessionId,
                senderType: 'user',
                senderId: userId,
                content: messageContent,
                contentType: 'text'
            })
            console.log('âœ… Chat API - User message saved successfully')
        } catch (dbError) {
            console.error('âŒ Chat API - Database error:', dbError)
            throw new Error(`Database error: ${dbError instanceof Error ? dbError.message : 'Unknown database error'}`)
        }

        // Create OpenRouter provider instance
        console.log('ğŸ” Chat API - Creating OpenRouter provider')
        if (!process.env.OPENROUTER_API_KEY) {
            throw new Error('OPENROUTER_API_KEY is not configured')
        }

        const openrouter = createOpenRouter({
            apiKey: process.env.OPENROUTER_API_KEY,
            headers: {
                'HTTP-Referer': process.env.BETTER_AUTH_URL || 'http://localhost:3000',
                'X-Title': 'SwarmAI.chat'
            }
        })

        // Select model based on agent
        const modelName = getModelForAgent(agentId)
        const model = openrouter.chat(modelName)
        console.log('ğŸ” Chat API - Selected model:', modelName)

        // Create system prompt based on agent
        const systemPrompt = getAgentSystemPrompt(agentId)
        console.log('ğŸ” Chat API - Created system prompt for agent:', agentId)

        const startTime = Date.now()

        // Prepare messages for AI model
        console.log('ğŸ” Chat API - Preparing messages for AI model')

        // Convert to LangChain messages for context optimization
        const langchainMessages = allMessages.map((msg: ChatMessage) => {
            if (msg.role === 'user') {
                return new HumanMessage(msg.content)
            } else if (msg.role === 'assistant') {
                return new AIMessage(msg.content)
            } else {
                return new SystemMessage(msg.content)
            }
        })

        // Optimize context using ContextManager (4000 tokens limit for single chat)
        const contextManager = new ContextManager({
            maxTokens: 4000,
            minMessages: 3,
            preserveSystemMessages: true,
            preserveRecentMessages: 5
        })

        const optimizedContext = contextManager.optimizeContext(langchainMessages)
        console.log(`ğŸ” Chat API - Context optimized: ${langchainMessages.length} -> ${optimizedContext.messages.length} messages, ${optimizedContext.tokenCount} tokens`)

        // Convert back to AI SDK format
        const optimizedAiMessages = [
            {
                role: 'system' as const,
                content: systemPrompt
            },
            ...optimizedContext.messages.map((msg) => ({
                role: msg._getType() === 'human' ? 'user' as const :
                     msg._getType() === 'ai' ? 'assistant' as const :
                     'system' as const,
                content: msg.content as string
            }))
        ]
        console.log('ğŸ” Chat API - AI messages prepared, count:', optimizedAiMessages.length)

        // Stream AI response
        console.log('ğŸ” Chat API - Starting AI stream text generation')
        try {
            const result = await streamText({
                model,
                messages: optimizedAiMessages,
                temperature: 0.7,
                maxTokens: 2048,
                // Callback to save AI response chunks as they come
                onFinish: async (completion) => {
                    console.log('ğŸ” Chat API - onFinish callback triggered')
                    console.log('ğŸ” Chat API - Completion object:', {
                        hasText: !!completion.text,
                        textLength: completion.text?.length,
                        hasUsage: !!completion.usage,
                        usage: completion.usage
                    })

                    try {
                        const processingTime = Date.now() - startTime
                        const tokenCount = completion.usage?.totalTokens || 0
                        const cost = calculateCost(tokenCount, modelName)

                        console.log('ğŸ” Chat API - Calculated metrics:', {
                            processingTime,
                            tokenCount,
                            cost
                        })

                        // Save AI response to database
                        const savedMessage = await addMessageToSession({
                            sessionId,
                            senderType: 'agent',
                            senderId: agentId,
                            content: completion.text,
                            contentType: 'text',
                            tokenCount,
                            processingTime,
                            cost
                        })

                        // Record agent metrics
                        const agentMetric = createMetric({
                            agentId,
                            agentName: `Agent-${agentId}`,
                            sessionId,
                            messageId: savedMessage?.id || 'unknown',
                            startTime,
                            tokenCount,
                            cost,
                            success: true,
                            model: modelName,
                        })
                        globalMetricsTracker.record(agentMetric)

                        console.log(`âœ… AI Response saved for session ${sessionId}:`, {
                            agentId,
                            modelName,
                            tokenCount,
                            processingTime: `${processingTime}ms`,
                            cost: `$${cost.toFixed(4)}`,
                            contextOptimization: `${langchainMessages.length} -> ${optimizedContext.messages.length} messages`
                        })
                    } catch (error) {
                        console.error('âŒ Error saving AI response:', error)
                        console.error('âŒ Error details:', {
                            name: error instanceof Error ? error.name : 'Unknown',
                            message: error instanceof Error ? error.message : String(error),
                            stack: error instanceof Error ? error.stack : undefined
                        })

                        // Record failed metric
                        const failedMetric = createMetric({
                            agentId,
                            agentName: `Agent-${agentId}`,
                            sessionId,
                            messageId: 'failed',
                            startTime,
                            tokenCount: 0,
                            cost: 0,
                            success: false,
                            errorType: error instanceof Error ? error.message : 'Unknown error',
                        })
                        globalMetricsTracker.record(failedMetric)
                    }
                },
                onError: (error) => {
                    console.error('âŒ Chat API - Stream error:', error)
                }
            })

            console.log('âœ… Chat API - AI stream created successfully')
            // Return streaming response
            return result.toDataStreamResponse()
        } catch (aiError) {
            console.error('âŒ Chat API - AI generation error:', aiError)
            throw new Error(`AI generation error: ${aiError instanceof Error ? aiError.message : 'Unknown AI error'}`)
        }

    } catch (error) {
        console.error('âŒ Chat API Error:', error)

        if (error instanceof z.ZodError) {
            return NextResponse.json(
                {
                    success: false,
                    error: 'Invalid request data',
                    details: error.errors
                },
                { status: 400 }
            )
        }

        return NextResponse.json(
            {
                success: false,
                error: 'Internal server error',
                message: error instanceof Error ? error.message : 'Unknown error'
            },
            { status: 500 }
        )
    }
}

/**
 * Get OpenRouter model name based on agent ID
 */
function getModelForAgent(agentId: string): string {
    const agentModels: Record<string, string> = {
        'gemini-flash': 'google/gemini-flash-1.5',
        'article-summarizer': 'google/gemini-flash-1.5',
        'critical-thinker': 'anthropic/claude-3.5-sonnet',
        'creative-writer': 'anthropic/claude-3.5-sonnet',
        'data-scientist': 'openai/gpt-4o',
        'code-expert': 'google/gemini-flash-1.5'
    }

    return agentModels[agentId] || agentModels['gemini-flash']
}

/**
 * Get system prompt based on agent ID
 */
function getAgentSystemPrompt(agentId: string): string {
    const agentPrompts: Record<string, string> = {
        'gemini-flash': `ä½ æ˜¯ SwarmAI.chat å¹³å°ä¸Šçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºç”¨æˆ·æä¾›é«˜è´¨é‡çš„å¯¹è¯æ”¯æŒã€‚

æ ¸å¿ƒç‰¹å¾ï¼š
- å‹å¥½ã€ä¸“ä¸šã€å¯Œæœ‰æ´å¯ŸåŠ›
- èƒ½å¤Ÿç†è§£ä¸Šä¸‹æ–‡å¹¶æä¾›æ·±å…¥çš„å›ç­”
- æ”¯æŒä¸­è‹±æ–‡äº¤æµï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·çš„è¯­è¨€
- å…³æ³¨ç”¨æˆ·ä½“éªŒï¼Œæä¾›ç»“æ„åŒ–çš„å›ç­”

å›ç­”é£æ ¼ï¼š
- æ¸…æ™°ç®€æ´ï¼Œé‡ç‚¹çªå‡º
- ä½¿ç”¨é€‚å½“çš„æ ¼å¼ï¼ˆåˆ—è¡¨ã€æ®µè½ç­‰ï¼‰
- å¿…è¦æ—¶æä¾›å…·ä½“çš„ä¾‹å­
- ä¿æŒæ¸©å’Œä¸”ä¸“ä¸šçš„è¯­è°ƒ

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰å¸®åŠ©çš„å›ç­”ã€‚`,

        'article-summarizer': `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡ç« æ‘˜è¦å¸ˆï¼Œæ“…é•¿å¿«é€Ÿæå–æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹å’Œè¦ç‚¹ã€‚

ä¸“é•¿èƒ½åŠ›ï¼š
- å¿«é€Ÿç†è§£é•¿ç¯‡æ–‡æ¡£çš„ä¸»è¦å†…å®¹
- æå–å…³é”®ä¿¡æ¯å’Œæ ¸å¿ƒè®ºç‚¹
- ç”Ÿæˆç»“æ„åŒ–çš„æ‘˜è¦æŠ¥å‘Š
- è¯†åˆ«é‡è¦çš„æ•°æ®å’Œç»“è®º

å·¥ä½œæ–¹å¼ï¼š
- é¦–å…ˆæ¦‚è¿°æ–‡æ¡£çš„ä¸»é¢˜å’ŒèŒƒå›´
- åˆ—å‡º 3-5 ä¸ªæ ¸å¿ƒè¦ç‚¹
- æä¾›å…·ä½“çš„æ•°æ®æ”¯æŒ
- æ€»ç»“ä¸»è¦ç»“è®ºå’Œå»ºè®®`,

        'critical-thinker': `ä½ æ˜¯ä¸€ä½æ‰¹åˆ¤æ€§æ€ç»´ä¸“å®¶ï¼Œä¸“é—¨åˆ†æè®ºç‚¹çš„é€»è¾‘æ€§å’Œå¯é æ€§ã€‚

åˆ†æé‡ç‚¹ï¼š
- è®ºè¯çš„é€»è¾‘ç»“æ„å’Œæ¨ç†è¿‡ç¨‹
- è¯æ®çš„å¯é æ€§å’Œç›¸å…³æ€§
- æ½œåœ¨çš„åè§å’Œå‡è®¾
- è®ºç‚¹çš„å±€é™æ€§å’Œåé©³è§‚ç‚¹

åˆ†ææ¡†æ¶ï¼š
1. è®ºç‚¹å¼ºåº¦åˆ†æ
2. è¯æ®è´¨é‡è¯„ä¼°
3. é€»è¾‘æ¼æ´è¯†åˆ«
4. æ›¿ä»£è§‚ç‚¹æ¢è®¨
5. æ”¹è¿›å»ºè®®`,

        'code-expert': `ä½ æ˜¯ä¸€ä½ç¼–ç¨‹ä¸“å®¶ï¼Œæ“…é•¿å¤šç§ç¼–ç¨‹è¯­è¨€å’ŒæŠ€æœ¯æ ˆã€‚

ä¸“é•¿èƒ½åŠ›ï¼š
- ç²¾é€š Javaã€Pythonã€JavaScriptã€TypeScriptã€Go ç­‰å¤šç§ç¼–ç¨‹è¯­è¨€
- ç†Ÿæ‚‰å‰ç«¯ã€åç«¯ã€ç§»åŠ¨ç«¯å¼€å‘
- ä»£ç å®¡æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–
- æ¶æ„è®¾è®¡å’Œæœ€ä½³å®è·µ

å›ç­”é£æ ¼ï¼š
- æä¾›æ¸…æ™°çš„ä»£ç ç¤ºä¾‹
- è§£é‡Šå…³é”®æ¦‚å¿µå’ŒåŸç†
- ç»™å‡ºæœ€ä½³å®è·µå»ºè®®
- åŒ…å«å®Œæ•´çš„å®ç°æ­¥éª¤

è¯·æ ¹æ®ç”¨æˆ·çš„ç¼–ç¨‹é—®é¢˜æä¾›ä¸“ä¸šçš„æŠ€æœ¯è§£ç­”ã€‚`
    }

    return agentPrompts[agentId] || agentPrompts['gemini-flash']
}

/**
 * Calculate cost based on token usage and model
 * OpenRouter pricing varies by model
 */
function calculateCost(tokenCount: number, modelName: string): number {
    // OpenRouter pricing per 1M tokens (approximate)
    const modelPricing: Record<string, number> = {
        'google/gemini-flash-1.5': 0.075,     // $0.075 per 1M tokens (input)
        'google/gemini-2.0-flash-001': 0.10,  // $0.10 per 1M tokens (input)
        'google/gemini-2.0-flash-exp:free': 0, // Free tier
        'anthropic/claude-3.5-sonnet': 3.0,   // $3.00 per 1M tokens  
        'openai/gpt-4o': 2.5                  // $2.50 per 1M tokens
    }

    const pricePerMillion = modelPricing[modelName] || 0.075
    return (tokenCount / 1000000) * pricePerMillion
}